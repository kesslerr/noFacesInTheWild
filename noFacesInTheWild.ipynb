{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "np.random.seed(42)\n",
    "from PIL import Image # to handle images\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.stats import ttest_ind\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPool2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and process to the format\n",
    "\n",
    "def augment_image(images4D):\n",
    "    ori_length = images4D.shape[0]\n",
    "    images4DAug = np.concatenate( (images4D, images4D, images4D, images4D), axis = 0 ) # quadruple it\n",
    "    images4DAug[ori_length*2:,:,:,:] = 255 - images4DAug[ori_length*2:,:,:,:] #invert the last half\n",
    "    images4DAug[ori_length:ori_length*2,:,:,:] = np.flip(images4DAug[ori_length:ori_length*2,:,:,:], axis = 2) # flip the second quarter\n",
    "    images4DAug[ori_length*3:ori_length*4,:,:,:] = np.flip(images4DAug[ori_length*3:ori_length*4,:,:,:], axis = 2)# flip the fourth quarter\n",
    "    return images4DAug\n",
    "\n",
    "def render_noisy(images4D, p=0.2):\n",
    "    na,nb,nc,nd = images4D.shape\n",
    "    for a in range(na):\n",
    "        for b in range(nb):\n",
    "            for c in range(nc):\n",
    "                r = np.random.binomial(1,p)\n",
    "                if r == 1:\n",
    "                    images4D[a,b,c,0] = 255-images4D[a,b,c,0]\n",
    "                    images4D[a,b,c,1] = 255-images4D[a,b,c,1]\n",
    "                    images4D[a,b,c,2] = 255-images4D[a,b,c,2]\n",
    "    return images4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 69.      75.      80.      89.     109.     132.     149.     177.3125\n",
      " 185.    ]\n"
     ]
    }
   ],
   "source": [
    "images = glob(\"D:/paredolia/noFacesInTheWild/data/wtface/small/*.jpg\")\n",
    "\n",
    "quantile_alphas = np.linspace(0.25,0.75,num=9) # define the alphas of the quantiles\n",
    "quantiles = np.quantile(img,quantile_alphas) # get the values (z) of the quantiles\n",
    "print(quantiles)\n",
    "\n",
    "Train = np.ndarray((len(images)*len(quantiles),43,32,3))\n",
    "\n",
    "counter = 0\n",
    "for j, file in enumerate(images):\n",
    "    try:\n",
    "        img = cv2.imread(file,0) # load\n",
    "        img = cv2.resize(img,(32,43)) # reshape it\n",
    "        img\n",
    "\n",
    "        for q in quantiles:\n",
    "            img2 = cv2.threshold(img,q,255,cv2.THRESH_BINARY)\n",
    "            img2 = cv2.cvtColor(img2[1],cv2.COLOR_GRAY2RGB) # to rgb #np.float32\n",
    "            \n",
    "            #print(np.min(img2))\n",
    "            Train[counter,:,:,:] = img2\n",
    "            counter += 1\n",
    "            \n",
    "    except:\n",
    "        print(\"error in image #\"+str(j))\n",
    "\n",
    "Train = augment_image(Train) # invert + mirror on vertical axis\n",
    "Train = np.concatenate( (Train, np.flip(Train, axis = 1)), axis = 0 ) # add a horizontally flipped control condition ( no pareidolia )\n",
    "X = Train\n",
    "y = np.repeat([1,0],Train.shape[0]/2)\n",
    "\n",
    "#Train = render_noisy(Train, p = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 15:50:08.053850  9808 image.py:709] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 15:50:08.212961  9808 image.py:709] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(2):\n",
    "    i = np.random.randint(len(Train[:,0,0,0]))\n",
    "    plt.imshow(Train[i,:,:,:]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87624, 43, 32, 3)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87624"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=15, ###  changed this\n",
    "    width_shift_range=0.1, ### changed this\n",
    "    height_shift_range=0.1, ### changed this\n",
    "    brightness_range=None,\n",
    "    shear_range=0.1, ### changed this\n",
    "    zoom_range=0.1, ### changed this\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0.0, \n",
    "    dtype=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterator\n",
    "X_train, X_test, y_train, y_test = train_test_split(Train, to_categorical(y), test_size = 0.25, random_state = 42)\n",
    "\n",
    "it = datagen.flow(X_train, y_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning():\n",
    "    # get mobilenet and add some layers at the end, 2 output classes\n",
    "    model = Sequential()\n",
    "    model.add(MobileNet(weights='imagenet',include_top=False, input_shape = (43, 32, 3) )) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax')) # sigmoid\n",
    "    #model.summary()\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # adam\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\roman\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras_applications\\mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 0.7184 - acc: 0.5193\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 110s 73ms/step - loss: 0.6741 - acc: 0.5833\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.6572 - acc: 0.6073\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 102s 68ms/step - loss: 0.6553 - acc: 0.6110\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 109s 73ms/step - loss: 0.6526 - acc: 0.6117\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 111s 74ms/step - loss: 0.6554 - acc: 0.6104\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 111s 74ms/step - loss: 0.6408 - acc: 0.6301\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 106s 70ms/step - loss: 0.6331 - acc: 0.6370\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 99s 66ms/step - loss: 0.6267 - acc: 0.6459\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 103s 68ms/step - loss: 0.6202 - acc: 0.6496\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 100s 67ms/step - loss: 0.6093 - acc: 0.6614\n",
      "Epoch 12/25\n",
      "1500/1500 [==============================] - 99s 66ms/step - loss: 0.6014 - acc: 0.6678\n",
      "Epoch 13/25\n",
      "1500/1500 [==============================] - 100s 66ms/step - loss: 0.5916 - acc: 0.6778\n",
      "Epoch 14/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.5813 - acc: 0.6852\n",
      "Epoch 15/25\n",
      "1500/1500 [==============================] - 100s 67ms/step - loss: 0.5699 - acc: 0.6935\n",
      "Epoch 16/25\n",
      "1500/1500 [==============================] - 104s 69ms/step - loss: 0.5606 - acc: 0.7033\n",
      "Epoch 17/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.5457 - acc: 0.7147\n",
      "Epoch 18/25\n",
      "1500/1500 [==============================] - 100s 67ms/step - loss: 0.5362 - acc: 0.72221s - l\n",
      "Epoch 19/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.5223 - acc: 0.73140s - loss: 0.5223 - acc: 0.731\n",
      "Epoch 20/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.5110 - acc: 0.7380\n",
      "Epoch 21/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.5011 - acc: 0.7481\n",
      "Epoch 22/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.4914 - acc: 0.7543\n",
      "Epoch 23/25\n",
      "1500/1500 [==============================] - 101s 68ms/step - loss: 0.4769 - acc: 0.7659\n",
      "Epoch 24/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.4668 - acc: 0.7708\n",
      "Epoch 25/25\n",
      "1500/1500 [==============================] - 101s 67ms/step - loss: 0.4535 - acc: 0.7790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25018d1a128>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transfer_learning()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # adam #categorical_crossentropy\n",
    "model.fit_generator(it, epochs=25, steps_per_epoch=1500) #, \n",
    "# 25 epochs should already lead to conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 79.33%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and test the subject data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_noise(model, X_test):\n",
    "    predictions = model.predict_proba(X_test)\n",
    "    facePredictions = predictions[:,1]\n",
    "    plt.plot(facePredictions)\n",
    "    plt.show()\n",
    "\n",
    "def load_labels_and_stim(s, sigma = 1): # s: subject identifier\n",
    "    # load behavior file\n",
    "    behaviorFile = 'D:/paredolia/facepareidolia/labels/' + s + 'df.csv'\n",
    "    df = pd.read_csv(behaviorFile)\n",
    "\n",
    "    # load stimuli\n",
    "\n",
    "    files = sorted(glob('D:/paredolia/facepareidolia/stimuli/'+ s +'/' + '*.tif'))\n",
    "\n",
    "    X = np.ndarray((len(files),43,32,3))\n",
    "    yn = []\n",
    "    c = 0\n",
    "    for j, file in enumerate(files):\n",
    "        fname = os.path.split(os.path.normpath(file))[-1] # file name without path\n",
    "\n",
    "        # check if face or noface\n",
    "        try:\n",
    "            if df.loc[df['picture'] == fname, 'category'].iloc[0] == \"Face\":\n",
    "                yn.append(1)\n",
    "            elif df.loc[df['picture'] == fname, 'category'].iloc[0] == \"noFace\":\n",
    "                yn.append(0)\n",
    "        except:\n",
    "            #print(fname  + \"not rated !\")\n",
    "            c += 1\n",
    "            continue\n",
    "        img = cv2.imread(file,0)\n",
    "        img = cv2.resize(img,(32,43)) # reshape it\n",
    "        img = np.flip(img, axis = 0) # faces are upside down\n",
    "        img = cv2.GaussianBlur(img,(sigma,sigma),0)\n",
    "        img = cv2.threshold(img,127,255,cv2.THRESH_BINARY) #TTT\n",
    "        img = cv2.cvtColor(img[1],cv2.COLOR_GRAY2RGB) # to rgb\n",
    "        img = np.expand_dims(img, 0) # new dimension\n",
    "        X[j-c,:,:,:] = img.astype(\"int32\")\n",
    "    X = np.delete(X, list(range(X.shape[0]-c , X.shape[0])), axis = 0) # delete the images which were not rated\n",
    "    return X, yn\n",
    "\n",
    "def test_model(model, sigma = 1):\n",
    "    \n",
    "    # determine subjects\n",
    "    subs = glob(\"D:/paredolia/facepareidolia/stimuli/*\")\n",
    "    subs = [os.path.split(subs[i])[1] for i,s in enumerate(subs)]\n",
    "    \n",
    "    # loop over subjects\n",
    "    allFacePrediction = np.ndarray(1)\n",
    "    allNoFacePrediction = np.ndarray(1)\n",
    "    for i,s in enumerate(subs):\n",
    "        #print('subject ' + s)\n",
    "        \n",
    "        # load stimuli and labels\n",
    "        X, yn = load_labels_and_stim(s, sigma = sigma)\n",
    "        \n",
    "        # predict\n",
    "        predictions = model.predict_proba(X)\n",
    "        facePredictions = predictions[:,1]\n",
    "        #plt.plot(facePredictions)\n",
    "        #plt.show()\n",
    "        indices_face   = [i for i, x in enumerate(yn) if x == 1]\n",
    "        indices_noface = [i for i, x in enumerate(yn) if x == 0]\n",
    "        \n",
    "        # test significance\n",
    "        print(ttest_ind(facePredictions[indices_face], facePredictions[indices_noface], equal_var=True))\n",
    "        allFacePrediction = np.concatenate([allFacePrediction,facePredictions[indices_face]],axis=0)\n",
    "        allNoFacePrediction = np.concatenate([allNoFacePrediction,facePredictions[indices_noface]],axis=0)\n",
    "        # test significance for the 100 strongest images\n",
    "        \n",
    "    # test significance across subjects !!\n",
    "    print(\"across all subjects:\")\n",
    "    print(ttest_ind(allFacePrediction, allNoFacePrediction, equal_var=True))\n",
    "    \n",
    "def compare_prediction_similarities(model, sigma = 1):\n",
    "    \n",
    "    # determine subjects\n",
    "    subs = glob(\"D:/paredolia/facepareidolia/stimuli/*\")\n",
    "    subs = [os.path.split(subs[i])[1] for i,s in enumerate(subs)]\n",
    "    \n",
    "    # loop over subjects\n",
    "    predAllSub = []\n",
    "    labelAllSub = []\n",
    "    for i,s in enumerate(subs):\n",
    "        print('subject ' + s)\n",
    "        \n",
    "        # load stimuli and labels\n",
    "        X, yn = load_labels_and_stim(s, sigma = sigma)\n",
    "        \n",
    "        # predict\n",
    "        predictions = model.predict_proba(X)\n",
    "        facePredictions = predictions[:,1]\n",
    "        \n",
    "        predAllSub.append(facePredictions)\n",
    "        labelAllSub.append(yn)\n",
    "        \n",
    "    return predAllSub, labelAllSub\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=2.548893037502808, pvalue=0.010851532765152983)\n",
      "Ttest_indResult(statistic=1.0708588987561427, pvalue=0.28431055615935186)\n",
      "Ttest_indResult(statistic=-2.2285309478698525, pvalue=0.026068577428092488)\n",
      "Ttest_indResult(statistic=-0.9321994978749331, pvalue=0.3514591177415586)\n",
      "Ttest_indResult(statistic=0.1626475603786006, pvalue=0.870829129651843)\n",
      "Ttest_indResult(statistic=1.4690830266571522, pvalue=0.14212603711355962)\n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, sigma = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, sigma = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labs = compare_prediction_similarities(model, sigma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = glob(\"D:/paredolia/facepareidolia/stimuli/*\")\n",
    "subs = [os.path.split(subs[i])[1] for i,s in enumerate(subs)]\n",
    "\n",
    "# to tidy df\n",
    "d = []\n",
    "sub = []\n",
    "lab = []\n",
    "for s, i in enumerate(preds):\n",
    "    l = labs[s]\n",
    "    for j,k in zip (i,l):\n",
    "        d.append(j)\n",
    "        sub.append(subs[s])\n",
    "        lab.append(k)\n",
    "di = {\"subject\":sub,\"face prediction\":d,\"label\":lab}\n",
    "\n",
    "df = pd.DataFrame(di)\n",
    "df.head()\n",
    "\n",
    "sns.stripplot(x=\"subject\", y = \"face prediction\", data = df, size=2)\n",
    "#sns.violinplot(x=\"subject\", y = \"face prediction\", data = df)\n",
    "#plt.yscale('logit') #log\n",
    "#plt.yscale('log') #log\n",
    "sns.violinplot(x=\"subject\", y = \"face prediction\", data = df)\n",
    "plt.axhline(y=0.5, color = \"black\")\n",
    "plt.ylim(0,1)\n",
    "#plt.show()\n",
    "#plt.savefig(\"./paper/face_predictions.png\", dpi=250, transparency = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
